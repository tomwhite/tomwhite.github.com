---
layout: post
title: Elastic Hadoop Clusters with Amazon's Elastic Block Store
date: '2008-08-23T21:39:00.002+01:00'
author: Tom White
tags:
- Amazon Web Services
- Hadoop
modified_time: '2008-08-23T21:41:35.517+01:00'
thumbnail: http://2.bp.blogspot.com/_IhqEHw4_Ick/SK7wd4umVGI/AAAAAAAAAE4/pnzP5XjfjtI/s72-c/s3-mapred.png
blogger_id: tag:blogger.com,1999:blog-8898949683610477251.post-7570203001448264185
blogger_orig_url: http://www.tom-e-white.com/2008/08/elastic-hadoop-clusters-with-amazons.html
---

I gave a <a href="http://skillsmatter.com/podcast/cloud-grid/hadoop-on-amazon-s3ec2">talk</a> on Tuesday at the first <a href="http://skillsmatter.com/event/java-jee/hadoop-user-group-meeting">Hadoop User Group UK</a> about Hadoop and Amazon Web services - how and why you can run Hadoop with AWS. I mentioned how integrating Hadoop with Amazon's "Persistent local storage", which Werner Vogels had <a href="http://www.allthingsdistributed.com/2008/04/persistent_storage_for_amazon.html">pre-announced</a> in April, would be a great feature to have to enable truly elastic Hadoop clusters that you could stop and start on demand.<br /><br />Well, the very next day Amazon launched this service, called <a href="http://www.amazon.com/b/ref=sc_fe_c_0_201590011_1?ie=UTF8&amp;node=689343011&amp;no=201590011">Elastic Block Store</a> (EBS). So in this post I thought I'd sketch out how an elastic Hadoop might work.<br /><br />A bit of background. Currently there are three main ways to use Hadoop with AWS:<br /><br /><h3>1. MapReduce with S3 source and sink</h3><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="{{ site.url }}/assets/2008-08-23-image-0004.png"><img style="margin: 0pt 10px 10px 0pt; float: right; cursor: pointer;" src="{{ site.url }}/assets/2008-08-23-image-0000.png" alt="" id="BLOGGER_PHOTO_ID_5237387812913173602" border="0"></a>In this set up, the data resides on S3, and the MapReduce daemons run on a temporary EC2 cluster for the duration of the job run. This works, and is especially convenient if you've already store your data on S3, but you don't get any data locality. Data locality is what enables the magic of MapReduce to work efficiently - the computation is scheduled to run on the machine where the data is stored, so you get huge savings in not having to ship terabytes of data around the network. EC2 does not share nodes with S3 storage, in fact they are often in different data centres, so performance is nowhere near as good as a regular Hadoop cluster where the data in stored in HDFS (see 3. below).<br /><br />It's not all doom and gloom, as the bandwidth between EC2 and S3 is actually pretty good, as Rightscale <a href="http://blog.rightscale.com/2007/10/28/network-performance-within-amazon-ec2-and-to-amazon-s3/">found when they did some measurements</a>.<br /><br /><h3>2. MapReduce from S3 with HDFS staging</h3><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="{{ site.url }}/assets/2008-08-23-image-0005.png"><img style="margin: 0pt 10px 10px 0pt; float: right; cursor: pointer;" src="{{ site.url }}/assets/2008-08-23-image-0001.png" alt="" id="BLOGGER_PHOTO_ID_5237388121957058018" border="0"></a>Data is stored on S3 but copied to a temporary HDFS cluster running on EC2. This is just a variation of the previous set-up, which is good if you want to run several jobs against the same input data. You save by only copying the data across the network once, but you pay a little more due to HDFS replication.<br /><br />The bottleneck is still copying the data out of S3. (Copying the results back into S3 isn't usually as bad as the output is often an order or two of magnitude smaller than the input.)<br /><br /><h3>3. HDFS on Amazon EC2</h3><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="{{ site.url }}/assets/2008-08-23-image-0006.png"><img style="margin: 0pt 10px 10px 0pt; float: right; cursor: pointer;" src="{{ site.url }}/assets/2008-08-23-image-0002.png" alt="" id="BLOGGER_PHOTO_ID_5237388381365626610" border="0"></a>Of course, you could just run a Hadoop cluster on EC2 and store your data there (and not in S3). In this scenario, you are committed to running your EC2 cluster long term, which can prove expensive, although the locality is excellent.<br /><br />These three scenarios demonstrate that you pay for locality. However, there is a gulf between S3 and local disks that EBS fills nicely. EBS does not have the bandwidth of local disks, but it's significantly better than S3. Rightscale <a href="http://blog.rightscale.com/2008/08/20/amazon-ebs-explained/">again</a>:<br /><blockquote><br />The bottom line though is that performance exceeds what weâ€™ve seen for filesystems striped across the four local drives of x-large instances.<br /></blockquote><br /><h3>Implementing Elastic Hadoop</h3><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="{{ site.url }}/assets/2008-08-23-image-0007.png"><img style="margin: 0pt 0pt 10px 10px; float: right; cursor: pointer;" src="{{ site.url }}/assets/2008-08-23-image-0003.png" alt="" id="BLOGGER_PHOTO_ID_5237388702578688562" border="0"></a>The main departure from the current Hadoop on EC2 approach is the need to maintain a map from storage volume to node type: i.e. we need to remember which volume is a master volume (storing the namenode's data) and which is a worker volume (storing the datanode's data). It would be nice if you could just start up EC2 instances for all the volumes, and have them figure out which is which, but this might not work as the master needs to be started first so its address can be given to the workers in their user data. (This choreography problem could be solved by introducing ZooKeeper, but that's another story.) So for a first cut, we could simply keep two files (locally, or on S3 or even SimpleDB) called <font style="font-style: italic;">master-volumes</font>, and <font style="font-style: italic;">worker-volumes</font>, which simply list the volume IDs for each node type, one per line.<br /><br />Assume there is one master running the namenode and jobtracker, and <font style="font-style: italic;">n</font> worker nodes each running a datanode and tasktracker.<br /><br />To create a new cluster<br /><ol><li>Create <font style="font-style: italic;">n</font> + 1 volumes.</li><li>Create the <font style="font-style: italic;">master-volumes</font> file and write the first volume ID into it.</li><li>Create the <font style="font-style: italic;">worker-volumes</font> file and write the remaining volume IDs to it.</li><li>Follow the steps for starting a cluster.</li></ol>To start a cluster<br /><ol><li>Start the master instance passing it the <font style="font-style: italic;">master-volumes</font> as user data. On startup the instance attaches to the volume it was told to. It then formats the namenode if it isn't already formatted, then starts the namenode, secondary namenode and jobtracker.</li><li>Start <span style="font-style: italic;">n</span> worker instances passing it the <font style="font-style: italic;">worker-volumes</font> as user data. On startup each instance attaches to the volume on line <font style="font-style: italic;">i</font>, where <font style="font-style: italic;">i</font> is the <font face="courier new">ami-launch-index</font> of the instance. Each instance then starts a datanode and tasktacker.</li><li>If any worker instances failed to start then launch them again.<br /></li></ol>To shutdown a cluster<br /><ol><li>Shutdown the Hadoop cluster daemons.</li><li>Detach the EBS volumes.</li><li>Shutdown the EC2 instances.</li></ol>To grow a cluster<br /><ol><li>Create <font style="font-style: italic;">m</font> new volumes, where <font style="font-style: italic;">m</font> is the size to grow by.</li><li>Append the <font style="font-style: italic;">m</font> new volume IDs to the <font style="font-style: italic;">worker-volumes</font> file.</li><li>Start <span style="font-style: italic;">m</span> worker instances passing it the <font style="font-style: italic;">worker-volumes</font> as user data. On startup each instance attaches to the volume on line <font style="font-style: italic;">n + i</font>, where <font style="font-style: italic;">i</font> is the <font face="courier new">ami-launch-index</font> of the instance. Each instance then starts a datanode and tasktacker.</li></ol>Future enhancements: attach multiple volumes for performance/storage growth on the namenode, or resilience on the namenode; integrate the secondary namenode backup facility with EBS snapshots to S3; provide tools for managing the <font style="font-style: italic;">worker-volumes</font> file (for example, integrating with datanode decommissioning).<br /><br />Building this would be a great project to work on - I hope someone does it!