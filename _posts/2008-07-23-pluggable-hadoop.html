---
layout: post
title: Pluggable Hadoop
date: '2008-07-23T20:26:00.002+01:00'
author: Tom White
tags:
- Hadoop
modified_time: '2008-07-23T22:18:34.869+01:00'
blogger_id: tag:blogger.com,1999:blog-8898949683610477251.post-1050541487495156879
blogger_orig_url: http://www.tom-e-white.com/2008/07/pluggable-hadoop.html
---

<span style="font-weight:bold;">Update</span>: This quote from Tim O'Reilly in his OSCON keynote today sums up the changes I describe below: "Do less and then create extensibility mechanisms." (via <a href="http://raibledesigns.com/rd/entry/oscon_2008_the_keynote">Matt Raible</a>)<br /><br />I'm noticing an increased desire to make Hadoop more modular. I'm not sure why this is happening now, but it's probably because as more people start using Hadoop it needs to be more malleable (people want to plug in their own implementations of things), and the way to do that in software is through modularity.<br /><br />Some examples:<br /><br /><h3>Job scheduling</h3>The current scheduler is a simple FIFO scheduler which is adequate for small clusters with a few cooperating users. On larger clusters the best advice has been to use <a href="http://hadoop.apache.org/core/docs/current/hod.html">HOD</a> (Hadoop On Demand), but that has its own problems with inefficient cluster utilization. This situation led to a number of proposals to make the scheduler pluggable (<a href="https://issues.apache.org/jira/browse/HADOOP-2510">HADOOP-2510</a>, <a href="https://issues.apache.org/jira/browse/HADOOP-3412">HADOOP-3412</a>, <a href="https://issues.apache.org/jira/browse/HADOOP-3444">HADOOP-3444</a>). Already there is a <a href="https://issues.apache.org/jira/browse/HADOOP-3746">fair scheduler implementation</a> (like the <a href="http://en.wikipedia.org/wiki/Completely_Fair_Scheduler">Completely Fair Scheduler</a> in Linux) from Facebook.<br /><br /><h3>HDFS block placement</h3>Today the algorithm for placing a file's blocks across datanodes in the cluster is hardcoded into HDFS, and while it has <a href="https://issues.apache.org/jira/browse/HADOOP-2559">evolved</a>, it is clear that a one-size-fits-all approach is not necessarily the best approach. Hence the new proposal to support <a href="https://issues.apache.org/jira/browse/HADOOP-3799">pluggable block placement algorithms</a>.<br /><br /><h3>Instrumentation</h3>Finding out what is happening in a distributed system is a hard problem. Today, Hadoop has a <a href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/metrics/package-summary.html">metrics API</a> (for gathering statistics from the main components of Hadoop), but there is interest in adding other logging systems, such as <a href="http://radlab.cs.berkeley.edu/wiki/Projects/X-Trace_on_Hadoop">X-Trace</a>, via a new <a href="https://issues.apache.org/jira/browse/HADOOP-3772">instrumentation API</a>.<br /><br /><h3>Serialization</h3>The ability to use <a href="https://issues.apache.org/jira/browse/HADOOP-1986">pluggable serialization frameworks</a> in MapReduce appeared in Hadoop 0.17.0, but has received renewed interest due to the talk around <a href="http://incubator.apache.org/thrift/">Apache Thrift</a> and <a href="http://code.google.com/p/protobuf/">Google Protocol Buffers</a>.<br /><br /><h3>Component lifecycle</h3>There is work being done to <a href="https://issues.apache.org/jira/browse/HADOOP-3628">add a lifecyle interface to Hadoop components</a>. One of the goals is to make it easier to subclass components, so they can be customized.<br /><br /><h3>Remove dependency cycles</h3>This is really just good engineering practice, but the existence of dependencies makes it harder to understand, modify and extend code. Bill de h√ìra did a great <a href="http://www.dehora.net/journal/2008/07/06/3-12-minutes-to-sort-a-terabyte-hadoops-code-structure/">analysis</a> of Hadoop's code structure (and its deficiencies), which has lead to some work to enforce module dependencies and remove the cycles.